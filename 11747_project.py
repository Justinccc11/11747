# -*- coding: utf-8 -*-
"""11747_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GieiY_-msB8RK_M2X8PZ9ELaAoAEhW6D
"""

!pip install datasets

# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it
import pyarrow
if int(pyarrow.__version__.split('.')[1]) < 16 and int(pyarrow.__version__.split('.')[0]) == 0:
    import os
    os.kill(os.getpid(), 9)

!pip install nlp

from datasets import list_datasets, list_metrics, load_dataset, load_metric

from pprint import pprint
import os
import sys
import json
import tensorflow as tf # Yes, we are going to play with Tensorflow 2!
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import absl # For using flags without tf.compat.v1.flags.Flag
import datetime

pip install apache_beam

pip install tensorflow==1.15.0

from google.colab import drive

drive.mount("/content/gdrive")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/11747 project

# Commented out IPython magic to ensure Python compatibility.
# %ls

!pip install tensorflow==2.0
!pip install tensorflow_hub
!pip install bert-for-tf2
!pip install sentencepiece

!pip install jsonlines

# Commented out IPython magic to ensure Python compatibility.
# from bert import run_classifier
# from bert import optimization
import bert
from bert import tokenization
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import hashlib
import glob
import os
from tensorflow.python.ops import math_ops
from collections import Counter

from tensorflow.python.keras.metrics import accuracy


# %matplotlib inline


BERT_MODEL_HUB = "https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1"

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/MyDrive

# Commented out IPython magic to ensure Python compatibility.
# %cd MyDrive/

# Commented out IPython magic to ensure Python compatibility.
# %cd 11747 project

# Commented out IPython magic to ensure Python compatibility.
# %ls

# Commented out IPython magic to ensure Python compatibility.
# %ls

dataset_file = "/content/gdrive/MyDrive/11747_project/v1.0-simplified_simplified-nq-train.jsonl"

!du -h {dataset_file}

import json
from pathlib import Path

path = Path('/content/gdrive/MyDrive/11747_project')
assert path.exists()
print(path/'single_line.json')

!ls -la {path}

!cd '/content/gdrive/MyDrive/11747_project'

dataset_file

!head -n 1 {dataset_file} > {'single_line.json'}

with open(path/'single_line.json') as f:
    line = f.readline()

line_dict = json.loads(line)

line_dict.keys()

line_dict['question_text']

line_dict['annotations']

AMOUNT_SENTENCES = 1

def shorten_context(context, answer):
    start_pos = context.find(answer)
    end_pos = context.find(answer) + len(answer)
    sentence_start = 0
    start_counter = 0
    for i in range(start_pos, -1, -1):
        if context[i] == '.':
            start_counter += 1
            if start_counter >= AMOUNT_SENTENCES:
                sentence_start = i + 1
                break
    sentence_end = len(context)
    end_counter = 0
    for i in range(end_pos + 1, sentence_end):
        if context[i] == '.':
            end_counter += 1
            if end_counter >= AMOUNT_SENTENCES:
                sentence_end = i
                break
    return context[sentence_start: sentence_end].strip()

def create_sentence(array):
#     res = ""
#     for s in reversed(array):
#         res =  (" " if len(s) > 1 or (s[0] not in string.punctuation or s[0] in ['(', ')', '-']) else "") + s + res
    sentence = ""
    for s in array:
        sentence += s + " "
    return sentence.strip()

def extract_short_answers(line_dict):
    answer_set = set([])
    question = line_dict['question_text'] + '?'
    context = create_sentence([t['token'] for t in line_dict['document_tokens'] if not t['html_token']])
    for anno in line_dict['annotations']:
        for sa in anno['short_answers']:
            tokens = [t for t in line_dict['document_tokens'][sa['start_token']:sa['end_token']] if not t['html_token']]
            answer = create_sentence([t['token'] for t in tokens])
            answer_set.add(answer)
    final_answers = []
    for a in answer_set:
        if context.find(a) > -1:
            short_context = shorten_context(context, a)
            if short_context.find(a) == -1:
                continue
            final_answers.append((question, a, {'answer_start': short_context.find(a), 'answer_end': short_context.find(a) + len(a)}, short_context))
    return final_answers

def navigate_data(full_data):
    answer_ds = []
    for i, l in enumerate(full_data):
        json_data = json.loads(l)
        for extracted in extract_short_answers(json_data):
            if len(extracted) > 0:
                answer_ds.append(extracted)
    return [a for a in answer_ds if len(a[1]) > 0]

def check_ds(answer_ds):
    count_not_found = []
    for i, a in enumerate(answer_ds):
        if a[2]['answer_start'] < 0:
            count_not_found.append((i, a))
        
    assert len(count_not_found) == 0

full_data = []
answer_datasets = []

with open("v1.0_sample_nq-train-sample.jsonl") as f:
    full_data = f.readlines()
    answer_ds = navigate_data(full_data)
    check_ds(answer_ds)        
    print(f'Extracted {len(answer_ds)} records out of {len(full_data)}')
    answer_datasets += answer_ds

answer_datasets

import pickle

pickle.dump(answer_datasets, open( "answers.pkl", "wb" ) )

!du -h answers.pkl

